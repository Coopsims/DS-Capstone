{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review Prediction - Deep Learning Models (PyTorch)\n",
    "**Author:** Ben\n",
    "\n",
    "**Project:** Capstone - Star Rating Prediction\n",
    "\n",
    "**Objective:** Train and evaluate deep learning models (RNN, LSTM, Hybrid CNN-LSTM) using PyTorch with CUDA acceleration\n",
    "\n",
    "---\n",
    "\n",
    "## Model Architectures\n",
    "\n",
    "**1. Basic RNN:** Simple recurrent network to establish sequential baseline\n",
    "\n",
    "**2. Bidirectional LSTM:** More sophisticated sequence model that can capture long-range dependencies and context from both directions\n",
    "\n",
    "**3. Hybrid CNN-LSTM:** Combines CNN for local feature extraction with LSTM for sequential patterns - often outperforms pure sequence models on text\n",
    "\n",
    "**Why PyTorch:**\n",
    "- More flexible than Keras for custom architectures\n",
    "- Better for research and experimentation\n",
    "- Excellent CUDA support for GPU acceleration\n",
    "- Industry standard for deep learning research"
   ],
   "id": "e1baae29fb2ec060"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ],
   "id": "a507d656d9728a3b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:00:29.860539200Z",
     "start_time": "2026-01-22T03:00:26.924742900Z"
    }
   },
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Sklearn for metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization defaults\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../Outputs/Models', exist_ok=True)\n",
    "os.makedirs('../Outputs/Plots', exist_ok=True)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ],
   "id": "85a68ff5d8a103ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "PyTorch version: 2.10.0+cu128\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:00:35.141788600Z",
     "start_time": "2026-01-22T03:00:35.106278600Z"
    }
   },
   "source": [
    "# Check CUDA availability and setup device\n",
    "print(\"=\" * 80)\n",
    "print(\"CUDA SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\\n✓ CUDA is available!\")\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"  Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"  Current GPU Memory:\")\n",
    "    print(f\"    Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"    Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\\n⚠️  CUDA is not available. Using CPU.\")\n",
    "    print(\"  Training will be significantly slower.\")\n",
    "    print(\"  Consider using Google Colab for free GPU access.\")\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"\\n✓ Random seed set to {SEED} for reproducibility\")"
   ],
   "id": "50e6e57b962dfa70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CUDA SETUP\n",
      "================================================================================\n",
      "\n",
      "✓ CUDA is available!\n",
      "  Device: NVIDIA GeForce RTX 5070 Ti\n",
      "  CUDA Version: 12.8\n",
      "  Number of GPUs: 1\n",
      "  Current GPU Memory:\n",
      "    Allocated: 0.00 GB\n",
      "    Cached: 0.00 GB\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "✓ Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ],
   "id": "292cda9474969029"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:01:31.658134200Z",
     "start_time": "2026-01-22T03:01:17.978108200Z"
    }
   },
   "source": [
    "# Load cleaned data\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use sample dataset for faster training during development\n",
    "USE_SAMPLE = False  # Set to False for full dataset\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    print(\"\\nLoading SAMPLE dataset for rapid iteration...\")\n",
    "    data_df = pd.read_csv('../Data/Processed/yelp_sample.csv')\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(data_df, test_size=0.2, \n",
    "                                         random_state=42, stratify=data_df['stars'])\n",
    "    # Further split train into train and validation\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.1, \n",
    "                                        random_state=42, stratify=train_df['stars'])\n",
    "else:\n",
    "    print(\"\\nLoading FULL dataset...\")\n",
    "    train_df = pd.read_csv('../Data/Processed/yelp_train.csv')\n",
    "    test_df = pd.read_csv('../Data/Processed/yelp_test.csv')\n",
    "    # Split off validation set from training\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.1, \n",
    "                                        random_state=42, stratify=train_df['stars'])\n",
    "\n",
    "print(f\"\\n✓ Data loaded:\")\n",
    "print(f\"  Train: {len(train_df):,} samples\")\n",
    "print(f\"  Validation: {len(val_df):,} samples\")\n",
    "print(f\"  Test: {len(test_df):,} samples\")\n",
    "\n",
    "# Target distribution\n",
    "print(\"\\nTarget distribution (train):\")\n",
    "print(train_df['stars'].value_counts(normalize=True).sort_index())"
   ],
   "id": "3fcb9ab8ceeb6267",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Loading FULL dataset...\n",
      "\n",
      "✓ Data loaded:\n",
      "  Train: 1,800,000 samples\n",
      "  Validation: 200,000 samples\n",
      "  Test: 500,000 samples\n",
      "\n",
      "Target distribution (train):\n",
      "stars\n",
      "1    0.2\n",
      "2    0.2\n",
      "3    0.2\n",
      "4    0.2\n",
      "5    0.2\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:02:09.438522400Z",
     "start_time": "2026-01-22T03:01:42.316498Z"
    }
   },
   "source": [
    "# Text preprocessing and tokenization\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEXT PREPROCESSING AND TOKENIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    \"\"\"Simple word tokenizer - split on whitespace and lowercase.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary from training data\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "all_words = []\n",
    "for text in train_df['text'].fillna(''):\n",
    "    all_words.extend(simple_tokenizer(text))\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "print(f\"  Total words: {len(all_words):,}\")\n",
    "print(f\"  Unique words: {len(word_counts):,}\")\n",
    "\n",
    "# Keep only words that appear at least MIN_FREQ times\n",
    "MIN_FREQ = 5\n",
    "vocab = {word for word, count in word_counts.items() if count >= MIN_FREQ}\n",
    "print(f\"  Vocabulary size (min_freq={MIN_FREQ}): {len(vocab):,}\")\n",
    "\n",
    "# Create word to index mapping\n",
    "# Reserve 0 for padding, 1 for unknown\n",
    "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "for idx, word in enumerate(sorted(vocab), start=2):\n",
    "    word2idx[word] = idx\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"\\n✓ Vocabulary created:\")\n",
    "print(f\"  Vocab size: {vocab_size:,}\")\n",
    "print(f\"  PAD token: {word2idx['<PAD>']}\")\n",
    "print(f\"  UNK token: {word2idx['<UNK>']}\")\n",
    "print(f\"\\nExample words and indices:\")\n",
    "for word in list(word2idx.keys())[2:12]:\n",
    "    print(f\"  '{word}': {word2idx[word]}\")"
   ],
   "id": "64db75bf10c13a4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEXT PREPROCESSING AND TOKENIZATION\n",
      "================================================================================\n",
      "\n",
      "Building vocabulary...\n",
      "  Total words: 210,703,898\n",
      "  Unique words: 1,525,450\n",
      "  Vocabulary size (min_freq=5): 245,597\n",
      "\n",
      "✓ Vocabulary created:\n",
      "  Vocab size: 245,599\n",
      "  PAD token: 0\n",
      "  UNK token: 1\n",
      "\n",
      "Example words and indices:\n",
      "  '!': 2\n",
      "  '!!': 3\n",
      "  '!!!': 4\n",
      "  '!!!!': 5\n",
      "  '!!!!!': 6\n",
      "  '!!!!!!': 7\n",
      "  '!!!!!!!': 8\n",
      "  '!!!!!!!!': 9\n",
      "  '!!!!!!!!!': 10\n",
      "  '!!!!!!!!!!': 11\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:05:25.154673900Z",
     "start_time": "2026-01-22T03:04:32.929119600Z"
    }
   },
   "source": [
    "# Convert text to sequences of indices\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONVERTING TEXT TO SEQUENCES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def text_to_sequence(text, word2idx, max_len=None):\n",
    "    \"\"\"\n",
    "    Convert text to sequence of word indices.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        word2idx: Word to index mapping\n",
    "        max_len: Maximum sequence length (truncate if longer)\n",
    "    \n",
    "    Returns:\n",
    "        List of word indices\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenizer(text)\n",
    "    sequence = [word2idx.get(word, word2idx['<UNK>']) for word in tokens]\n",
    "    \n",
    "    if max_len:\n",
    "        sequence = sequence[:max_len]\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Set maximum sequence length based on data\n",
    "MAX_LEN = 200  # Truncate reviews longer than 200 words\n",
    "\n",
    "print(f\"\\nConverting text to sequences (max_len={MAX_LEN})...\")\n",
    "train_sequences = [text_to_sequence(text, word2idx, MAX_LEN) \n",
    "                   for text in train_df['text'].fillna('')]\n",
    "val_sequences = [text_to_sequence(text, word2idx, MAX_LEN) \n",
    "                 for text in val_df['text'].fillna('')]\n",
    "test_sequences = [text_to_sequence(text, word2idx, MAX_LEN) \n",
    "                  for text in test_df['text'].fillna('')]\n",
    "\n",
    "# Get sequence lengths before padding\n",
    "train_lengths = [len(seq) for seq in train_sequences]\n",
    "val_lengths = [len(seq) for seq in val_sequences]\n",
    "test_lengths = [len(seq) for seq in test_sequences]\n",
    "\n",
    "print(f\"\\n✓ Sequences created\")\n",
    "print(f\"  Average length (before padding): {np.mean(train_lengths):.1f} words\")\n",
    "print(f\"  Median length: {np.median(train_lengths):.0f} words\")\n",
    "print(f\"  Max length (capped): {MAX_LEN} words\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nExample sequence (first 20 tokens):\")\n",
    "example_text = train_df.iloc[0]['text']\n",
    "example_seq = train_sequences[0]\n",
    "print(f\"  Original: {example_text[:100]}...\")\n",
    "print(f\"  Sequence: {example_seq[:20]}\")\n",
    "print(f\"  Decoded: {' '.join([idx2word[idx] for idx in example_seq[:20]])}\")"
   ],
   "id": "bc7170183c73e490",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONVERTING TEXT TO SEQUENCES\n",
      "================================================================================\n",
      "\n",
      "Converting text to sequences (max_len=200)...\n",
      "\n",
      "✓ Sequences created\n",
      "  Average length (before padding): 99.8 words\n",
      "  Median length: 85 words\n",
      "  Max length (capped): 200 words\n",
      "\n",
      "Example sequence (first 20 tokens):\n",
      "  Original: What use to be free is now $8, is it worth it? Kind of. You'll see cars here that you've never seen ...\n",
      "  Sequence: [239244, 232089, 223233, 47102, 104653, 127520, 158670, 11510, 127520, 127832, 242235, 128131, 132758, 160115, 244244, 197195, 61284, 117761, 219972, 244253]\n",
      "  Decoded: what use to be free is now $8, is it worth it? kind of. you'll see cars here that you've\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:06:28.448329600Z",
     "start_time": "2026-01-22T03:06:23.541528Z"
    }
   },
   "source": [
    "# Pad sequences and convert to tensors\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PADDING AND CREATING TENSORS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def pad_sequences(sequences, max_len, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pad sequences to the same length.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of sequences\n",
    "        max_len: Target length\n",
    "        pad_value: Value to use for padding\n",
    "    \n",
    "    Returns:\n",
    "        Padded numpy array\n",
    "    \"\"\"\n",
    "    padded = np.zeros((len(sequences), max_len), dtype=np.int64)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = min(len(seq), max_len)\n",
    "        padded[i, :length] = seq[:length]\n",
    "    \n",
    "    return padded\n",
    "\n",
    "# Pad sequences\n",
    "X_train = pad_sequences(train_sequences, MAX_LEN)\n",
    "X_val = pad_sequences(val_sequences, MAX_LEN)\n",
    "X_test = pad_sequences(test_sequences, MAX_LEN)\n",
    "\n",
    "# Convert labels to numpy (subtract 1 to make 0-indexed for PyTorch)\n",
    "y_train = train_df['stars'].values - 1  # Now 0-4 instead of 1-5\n",
    "y_val = val_df['stars'].values - 1\n",
    "y_test = test_df['stars'].values - 1\n",
    "\n",
    "print(f\"\\n✓ Data prepared:\")\n",
    "print(f\"  X_train shape: {X_train.shape}\")\n",
    "print(f\"  X_val shape: {X_val.shape}\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n",
    "print(f\"  y_train shape: {y_train.shape}\")\n",
    "print(f\"  Label range: {y_train.min()} to {y_train.max()} (0-indexed)\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.LongTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_val_tensor = torch.LongTensor(X_val)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "X_test_tensor = torch.LongTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "print(f\"\\n✓ Tensors created\")\n",
    "print(f\"  X_train_tensor: {X_train_tensor.shape}, dtype: {X_train_tensor.dtype}\")\n",
    "print(f\"  y_train_tensor: {y_train_tensor.shape}, dtype: {y_train_tensor.dtype}\")"
   ],
   "id": "86db6ee4492686ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PADDING AND CREATING TENSORS\n",
      "================================================================================\n",
      "\n",
      "✓ Data prepared:\n",
      "  X_train shape: (1800000, 200)\n",
      "  X_val shape: (200000, 200)\n",
      "  X_test shape: (500000, 200)\n",
      "  y_train shape: (1800000,)\n",
      "  Label range: 0 to 4 (0-indexed)\n",
      "\n",
      "✓ Tensors created\n",
      "  X_train_tensor: torch.Size([1800000, 200]), dtype: torch.int64\n",
      "  y_train_tensor: torch.Size([1800000]), dtype: torch.int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:06:39.562782100Z",
     "start_time": "2026-01-22T03:06:39.485342200Z"
    }
   },
   "source": [
    "# Create DataLoaders\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING DATALOADERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\n✓ DataLoaders created with batch_size={BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test DataLoader\n",
    "test_batch = next(iter(train_loader))\n",
    "print(f\"\\nTest batch:\")\n",
    "print(f\"  Input shape: {test_batch[0].shape}\")\n",
    "print(f\"  Label shape: {test_batch[1].shape}\")\n",
    "print(f\"  Sample input (first 5 tokens): {test_batch[0][0][:5]}\")\n",
    "print(f\"  Sample label: {test_batch[1][0]} (corresponds to {test_batch[1][0].item() + 1} stars)\")"
   ],
   "id": "66dde55f2410609",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING DATALOADERS\n",
      "================================================================================\n",
      "\n",
      "✓ DataLoaders created with batch_size=64\n",
      "  Train batches: 28125\n",
      "  Val batches: 3125\n",
      "  Test batches: 7813\n",
      "\n",
      "Test batch:\n",
      "  Input shape: torch.Size([64, 200])\n",
      "  Label shape: torch.Size([64])\n",
      "  Sample input (first 5 tokens): tensor([ 95435, 122476, 233874, 152914,  93023])\n",
      "  Sample label: 4 (corresponds to 5 stars)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architectures"
   ],
   "id": "288ec5bae8a6c152"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:06:50.351625Z",
     "start_time": "2026-01-22T03:06:50.326398700Z"
    }
   },
   "source": [
    "# Model 1: Basic RNN\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 1: BASIC RNN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class BasicRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN model for text classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding layer\n",
    "    - Vanilla RNN layer\n",
    "    - Dropout for regularization\n",
    "    - Fully connected output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.5):\n",
    "        super(BasicRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                         batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # output shape: [batch_size, seq_len, hidden_dim]\n",
    "        # hidden shape: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Use the final hidden state\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        # hidden shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        output = self.fc(hidden)\n",
    "        # output shape: [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"\\n✓ BasicRNN class defined\")\n",
    "print(\"\\nArchitecture: Embedding → RNN → Dropout → Linear\")"
   ],
   "id": "91a58854e7311472",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 1: BASIC RNN\n",
      "================================================================================\n",
      "\n",
      "✓ BasicRNN class defined\n",
      "\n",
      "Architecture: Embedding → RNN → Dropout → Linear\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:06:57.053852900Z",
     "start_time": "2026-01-22T03:06:57.028914600Z"
    }
   },
   "source": [
    "# Model 2: Bidirectional LSTM\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 2: BIDIRECTIONAL LSTM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM model for text classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding layer\n",
    "    - Bidirectional LSTM (reads sequence forwards and backwards)\n",
    "    - Dropout for regularization\n",
    "    - Fully connected output layer\n",
    "    \n",
    "    Bidirectional helps capture context from both directions.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=2, dropout=0.5):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                           batch_first=True, dropout=dropout if n_layers > 1 else 0,\n",
    "                           bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # *2 because bidirectional concatenates forward and backward hidden states\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # output shape: [batch_size, seq_len, hidden_dim * 2]\n",
    "        # hidden shape: [n_layers * 2, batch_size, hidden_dim]\n",
    "        \n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden shape: [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc(hidden)\n",
    "        # output shape: [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"\\n✓ BiLSTM class defined\")\n",
    "print(\"\\nArchitecture: Embedding → Bidirectional LSTM → Dropout → Linear\")\n",
    "print(\"Note: Bidirectional doubles the hidden dimension\")"
   ],
   "id": "f15bd7bd9ea7d334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 2: BIDIRECTIONAL LSTM\n",
      "================================================================================\n",
      "\n",
      "✓ BiLSTM class defined\n",
      "\n",
      "Architecture: Embedding → Bidirectional LSTM → Dropout → Linear\n",
      "Note: Bidirectional doubles the hidden dimension\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:07:03.171158600Z",
     "start_time": "2026-01-22T03:07:03.130653800Z"
    }
   },
   "source": [
    "# Model 3: Hybrid CNN-LSTM\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 3: HYBRID CNN-LSTM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid CNN-LSTM model for text classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding layer\n",
    "    - 1D Convolutional layers for local feature extraction\n",
    "    - Max pooling\n",
    "    - LSTM to capture sequential dependencies\n",
    "    - Fully connected output layer\n",
    "    \n",
    "    CNN extracts local n-gram features, LSTM captures longer dependencies.\n",
    "    Often outperforms pure RNN/LSTM on text classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_filters=100, filter_sizes=[3, 4, 5], dropout=0.5):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Multiple conv layers with different filter sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, \n",
    "                     out_channels=n_filters, \n",
    "                     kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # LSTM takes concatenated CNN outputs\n",
    "        self.lstm = nn.LSTM(n_filters * len(filter_sizes), hidden_dim, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Permute for conv1d: [batch_size, embedding_dim, seq_len]\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply convolution and relu to each filter size\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        # Each conved[i] shape: [batch_size, n_filters, seq_len - filter_size + 1]\n",
    "        \n",
    "        # Max pool over time dimension\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # Each pooled[i] shape: [batch_size, n_filters]\n",
    "        \n",
    "        # Concatenate all pooled outputs\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat shape: [batch_size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        # Reshape for LSTM: [batch_size, 1, features]\n",
    "        cat = cat.unsqueeze(1)\n",
    "        \n",
    "        # LSTM\n",
    "        output, (hidden, cell) = self.lstm(cat)\n",
    "        # Concatenate final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden shape: [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc(hidden)\n",
    "        # output shape: [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"\\n✓ CNN_LSTM class defined\")\n",
    "print(\"\\nArchitecture: Embedding → Conv1D (multiple filters) → MaxPool → \")\n",
    "print(\"              Bi-LSTM → Dropout → Linear\")\n",
    "print(\"\\nThis architecture combines:\")\n",
    "print(\"  - CNNs for local n-gram feature extraction\")\n",
    "print(\"  - LSTM for capturing longer-range dependencies\")"
   ],
   "id": "8a24b2323a054c9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3: HYBRID CNN-LSTM\n",
      "================================================================================\n",
      "\n",
      "✓ CNN_LSTM class defined\n",
      "\n",
      "Architecture: Embedding → Conv1D (multiple filters) → MaxPool → \n",
      "              Bi-LSTM → Dropout → Linear\n",
      "\n",
      "This architecture combines:\n",
      "  - CNNs for local n-gram feature extraction\n",
      "  - LSTM for capturing longer-range dependencies\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Functions"
   ],
   "id": "527e8bc647291496"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:07:07.452627Z",
     "start_time": "2026-01-22T03:07:07.429587800Z"
    }
   },
   "source": [
    "# Training and evaluation functions\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING UTILITIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def train_epoch(model, iterator, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        iterator: DataLoader\n",
    "        optimizer: Optimizer\n",
    "        criterion: Loss function\n",
    "        device: torch.device\n",
    "    \n",
    "    Returns:\n",
    "        Average loss and accuracy for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # Get batch data\n",
    "        text, labels = batch\n",
    "        text = text.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = (predictions.argmax(1) == labels).float().mean()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        iterator: DataLoader\n",
    "        criterion: Loss function\n",
    "        device: torch.device\n",
    "    \n",
    "    Returns:\n",
    "        Average loss and accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text = text.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = (predictions.argmax(1) == labels).float().mean()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def predict(model, iterator, device):\n",
    "    \"\"\"\n",
    "    Generate predictions for all samples.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        iterator: DataLoader\n",
    "        device: torch.device\n",
    "    \n",
    "    Returns:\n",
    "        Numpy arrays of predictions and true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text = text.to(device)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            preds = predictions.argmax(1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n✓ Training functions defined:\")\n",
    "print(\"  - train_epoch(): Train for one epoch\")\n",
    "print(\"  - evaluate(): Evaluate model performance\")\n",
    "print(\"  - predict(): Generate predictions\")\n",
    "print(\"  - count_parameters(): Count trainable parameters\")"
   ],
   "id": "64a2946370203367",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING UTILITIES\n",
      "================================================================================\n",
      "\n",
      "✓ Training functions defined:\n",
      "  - train_epoch(): Train for one epoch\n",
      "  - evaluate(): Evaluate model performance\n",
      "  - predict(): Generate predictions\n",
      "  - count_parameters(): Count trainable parameters\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ],
   "id": "fe615a038c9e01b1"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:07:14.323381200Z",
     "start_time": "2026-01-22T03:07:14.218463300Z"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPERPARAMETERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 5  # 5 classes (1-5 stars, 0-indexed as 0-4)\n",
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "hyperparams = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'output_dim': OUTPUT_DIM,\n",
    "    'n_epochs': N_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'max_len': MAX_LEN\n",
    "}\n",
    "\n",
    "print(\"\\nModel hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Loss function with class weights for imbalanced data\n",
    "print(\"\\nCalculating class weights for imbalanced dataset...\")\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"Class weights: {class_weights.cpu().numpy()}\")\n",
    "print(f\"\\n(Higher weight = rarer class, will be penalized more in loss)\")"
   ],
   "id": "a5e62d31f1848df7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HYPERPARAMETERS\n",
      "================================================================================\n",
      "\n",
      "Model hyperparameters:\n",
      "  vocab_size: 245599\n",
      "  embedding_dim: 100\n",
      "  hidden_dim: 256\n",
      "  output_dim: 5\n",
      "  n_epochs: 10\n",
      "  learning_rate: 0.001\n",
      "  batch_size: 64\n",
      "  max_len: 200\n",
      "\n",
      "Calculating class weights for imbalanced dataset...\n",
      "Class weights: [1. 1. 1. 1. 1.]\n",
      "\n",
      "(Higher weight = rarer class, will be penalized more in loss)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-22T03:07:18.813003600Z"
    }
   },
   "source": [
    "# Train Model 1: Basic RNN\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING MODEL 1: BASIC RNN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize model\n",
    "rnn_model = BasicRNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(rnn_model)\n",
    "print(f\"\\nTrainable parameters: {count_parameters(rnn_model):,}\")\n",
    "\n",
    "# Optimizer and loss\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nTraining for {N_EPOCHS} epochs...\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "rnn_train_losses = []\n",
    "rnn_train_accs = []\n",
    "rnn_val_losses = []\n",
    "rnn_val_accs = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(rnn_model, train_loader, rnn_optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(rnn_model, val_loader, criterion, device)\n",
    "    \n",
    "    rnn_train_losses.append(train_loss)\n",
    "    rnn_train_accs.append(train_acc)\n",
    "    rnn_val_losses.append(val_loss)\n",
    "    rnn_val_accs.append(val_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d}/{N_EPOCHS} | Time: {epoch_time:.1f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(rnn_model.state_dict(), '../Outputs/Models/rnn_best.pt')\n",
    "        print(f\"  → Saved new best model (val_loss: {val_loss:.4f})\")\n",
    "    print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✓ Training complete in {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Load best model for evaluation\n",
    "rnn_model.load_state_dict(torch.load('../Outputs/Models/rnn_best.pt'))\n",
    "print(f\"\\n✓ Loaded best model from checkpoint\")"
   ],
   "id": "2be3edcb06b96d11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 1: BASIC RNN\n",
      "================================================================================\n",
      "\n",
      "Model architecture:\n",
      "BasicRNN(\n",
      "  (embedding): Embedding(245599, 100, padding_idx=0)\n",
      "  (rnn): RNN(100, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      ")\n",
      "\n",
      "Trainable parameters: 24,784,417\n",
      "\n",
      "Training for 10 epochs...\n",
      "Using device: cuda\n",
      "\n",
      "Epoch 01/10 | Time: 165.3s\n",
      "  Train Loss: 1.6304 | Train Acc: 20.31%\n",
      "  Val Loss:   1.6138 | Val Acc:   22.16%\n",
      "  → Saved new best model (val_loss: 1.6138)\n",
      "\n",
      "Epoch 02/10 | Time: 173.1s\n",
      "  Train Loss: 1.6300 | Train Acc: 20.36%\n",
      "  Val Loss:   1.6313 | Val Acc:   20.77%\n",
      "\n",
      "Epoch 03/10 | Time: 166.1s\n",
      "  Train Loss: 1.6297 | Train Acc: 20.39%\n",
      "  Val Loss:   1.6278 | Val Acc:   21.95%\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train Model 2: Bidirectional LSTM\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING MODEL 2: BIDIRECTIONAL LSTM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize model\n",
    "lstm_model = BiLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(lstm_model)\n",
    "print(f\"\\nTrainable parameters: {count_parameters(lstm_model):,}\")\n",
    "\n",
    "# Optimizer and loss\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nTraining for {N_EPOCHS} epochs...\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "lstm_train_losses = []\n",
    "lstm_train_accs = []\n",
    "lstm_val_losses = []\n",
    "lstm_val_accs = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(lstm_model, train_loader, lstm_optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(lstm_model, val_loader, criterion, device)\n",
    "    \n",
    "    lstm_train_losses.append(train_loss)\n",
    "    lstm_train_accs.append(train_acc)\n",
    "    lstm_val_losses.append(val_loss)\n",
    "    lstm_val_accs.append(val_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d}/{N_EPOCHS} | Time: {epoch_time:.1f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(lstm_model.state_dict(), '../Outputs/Models/lstm_best.pt')\n",
    "        print(f\"  → Saved new best model (val_loss: {val_loss:.4f})\")\n",
    "    print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✓ Training complete in {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Load best model for evaluation\n",
    "lstm_model.load_state_dict(torch.load('../Outputs/Models/lstm_best.pt'))\n",
    "print(f\"\\n✓ Loaded best model from checkpoint\")"
   ],
   "id": "12d3556da3e7ee3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train Model 3: Hybrid CNN-LSTM\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING MODEL 3: HYBRID CNN-LSTM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize model\n",
    "cnn_lstm_model = CNN_LSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_filters=100,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(cnn_lstm_model)\n",
    "print(f\"\\nTrainable parameters: {count_parameters(cnn_lstm_model):,}\")\n",
    "\n",
    "# Optimizer and loss\n",
    "cnn_lstm_optimizer = optim.Adam(cnn_lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nTraining for {N_EPOCHS} epochs...\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "cnn_lstm_train_losses = []\n",
    "cnn_lstm_train_accs = []\n",
    "cnn_lstm_val_losses = []\n",
    "cnn_lstm_val_accs = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(cnn_lstm_model, train_loader, cnn_lstm_optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(cnn_lstm_model, val_loader, criterion, device)\n",
    "    \n",
    "    cnn_lstm_train_losses.append(train_loss)\n",
    "    cnn_lstm_train_accs.append(train_acc)\n",
    "    cnn_lstm_val_losses.append(val_loss)\n",
    "    cnn_lstm_val_accs.append(val_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d}/{N_EPOCHS} | Time: {epoch_time:.1f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(cnn_lstm_model.state_dict(), '../Outputs/Models/cnn_lstm_best.pt')\n",
    "        print(f\"  → Saved new best model (val_loss: {val_loss:.4f})\")\n",
    "    print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✓ Training complete in {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Load best model for evaluation\n",
    "cnn_lstm_model.load_state_dict(torch.load('../Outputs/Models/cnn_lstm_best.pt'))\n",
    "print(f\"\\n✓ Loaded best model from checkpoint\")"
   ],
   "id": "c438c33ca08b0ac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ],
   "id": "71bf73abac80fb09"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate all models on test set\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get predictions from all models\n",
    "print(\"\\nGenerating predictions...\")\n",
    "rnn_preds, y_test_np = predict(rnn_model, test_loader, device)\n",
    "lstm_preds, _ = predict(lstm_model, test_loader, device)\n",
    "cnn_lstm_preds, _ = predict(cnn_lstm_model, test_loader, device)\n",
    "\n",
    "# Calculate metrics\n",
    "models_results = [\n",
    "    ('Basic RNN', rnn_preds),\n",
    "    ('Bidirectional LSTM', lstm_preds),\n",
    "    ('Hybrid CNN-LSTM', cnn_lstm_preds)\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for model_name, preds in models_results:\n",
    "    accuracy = accuracy_score(y_test_np, preds)\n",
    "    f1_macro = f1_score(y_test_np, preds, average='macro')\n",
    "    f1_weighted = f1_score(y_test_np, preds, average='weighted')\n",
    "    \n",
    "    results_summary.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 (Macro)': f1_macro,\n",
    "        'F1 (Weighted)': f1_weighted\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  F1-Score (macro): {f1_macro:.4f}\")\n",
    "    print(f\"  F1-Score (weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\", results_df.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_idx = results_df['Accuracy'].idxmax()\n",
    "best_model_name = results_df.loc[best_idx, 'Model']\n",
    "best_accuracy = results_df.loc[best_idx, 'Accuracy']\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Test accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")"
   ],
   "id": "53371c7d8fa4f253",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Detailed classification reports\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "star_names = ['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars']\n",
    "\n",
    "for model_name, preds in models_results:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print('='*80)\n",
    "    print(classification_report(y_test_np, preds, target_names=star_names))"
   ],
   "id": "9b7a469c874157fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize confusion matrices\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (model_name, preds) in enumerate(models_results):\n",
    "    cm = confusion_matrix(y_test_np, preds)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "               xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5],\n",
    "               ax=axes[idx], cbar_kws={'label': 'Proportion'})\n",
    "    axes[idx].set_xlabel('Predicted Rating', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual Rating', fontweight='bold')\n",
    "    axes[idx].set_title(model_name, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Outputs/Plots/12_dl_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Plot saved: 12_dl_confusion_matrices.png\")"
   ],
   "id": "7d995cb78e50a234",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot training curves\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING CURVES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Training history for each model\n",
    "training_data = [\n",
    "    ('Basic RNN', rnn_train_losses, rnn_val_losses, rnn_train_accs, rnn_val_accs),\n",
    "    ('Bi-LSTM', lstm_train_losses, lstm_val_losses, lstm_train_accs, lstm_val_accs),\n",
    "    ('CNN-LSTM', cnn_lstm_train_losses, cnn_lstm_val_losses, cnn_lstm_train_accs, cnn_lstm_val_accs)\n",
    "]\n",
    "\n",
    "for idx, (name, train_loss, val_loss, train_acc, val_acc) in enumerate(training_data):\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, idx].plot(epochs, train_loss, 'b-', label='Training')\n",
    "    axes[0, idx].plot(epochs, val_loss, 'r-', label='Validation')\n",
    "    axes[0, idx].set_xlabel('Epoch', fontweight='bold')\n",
    "    axes[0, idx].set_ylabel('Loss', fontweight='bold')\n",
    "    axes[0, idx].set_title(f'{name} - Loss', fontweight='bold')\n",
    "    axes[0, idx].legend()\n",
    "    axes[0, idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1, idx].plot(epochs, [acc*100 for acc in train_acc], 'b-', label='Training')\n",
    "    axes[1, idx].plot(epochs, [acc*100 for acc in val_acc], 'r-', label='Validation')\n",
    "    axes[1, idx].set_xlabel('Epoch', fontweight='bold')\n",
    "    axes[1, idx].set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "    axes[1, idx].set_title(f'{name} - Accuracy', fontweight='bold')\n",
    "    axes[1, idx].legend()\n",
    "    axes[1, idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Outputs/Plots/13_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Plot saved: 13_training_curves.png\")"
   ],
   "id": "ebf4f0a13db31aca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].barh(results_df['Model'], results_df['Accuracy'], \n",
    "            color=['steelblue', 'darkblue', 'navy'])\n",
    "axes[0].set_xlabel('Accuracy', fontweight='bold')\n",
    "axes[0].set_title('Model Accuracy Comparison', fontweight='bold')\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    axes[0].text(v + 0.01, i, f'{v:.3f} ({v*100:.1f}%)', va='center', fontweight='bold')\n",
    "\n",
    "# F1 Score comparison\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, results_df['F1 (Macro)'], width, label='Macro F1', color='steelblue')\n",
    "axes[1].bar(x + width/2, results_df['F1 (Weighted)'], width, label='Weighted F1', color='darkblue')\n",
    "axes[1].set_xlabel('Model', fontweight='bold')\n",
    "axes[1].set_ylabel('F1 Score', fontweight='bold')\n",
    "axes[1].set_title('F1 Score Comparison', fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(results_df['Model'], rotation=15, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Outputs/Plots/14_dl_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Plot saved: 14_dl_model_comparison.png\")"
   ],
   "id": "36c037dad039a0ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ],
   "id": "ae34261860803d1c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save all results\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save results CSV\n",
    "results_df.to_csv('../Outputs/dl_model_comparison.csv', index=False)\n",
    "print(\"✓ Saved: dl_model_comparison.csv\")\n",
    "\n",
    "# Save vocabulary\n",
    "with open('../Outputs/Models/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump({'word2idx': word2idx, 'idx2word': idx2word}, f)\n",
    "print(\"✓ Saved: vocab.pkl\")\n",
    "\n",
    "# Save hyperparameters\n",
    "import json\n",
    "with open('../Outputs/dl_hyperparameters.json', 'w') as f:\n",
    "    json.dump(hyperparams, f, indent=2)\n",
    "print(\"✓ Saved: dl_hyperparameters.json\")\n",
    "\n",
    "# Save detailed results\n",
    "detailed_results = {\n",
    "    'models': [],\n",
    "    'best_model': best_model_name,\n",
    "    'best_accuracy': float(best_accuracy)\n",
    "}\n",
    "\n",
    "for model_name, preds in models_results:\n",
    "    model_results = {\n",
    "        'name': model_name,\n",
    "        'accuracy': float(accuracy_score(y_test_np, preds)),\n",
    "        'f1_macro': float(f1_score(y_test_np, preds, average='macro')),\n",
    "        'f1_weighted': float(f1_score(y_test_np, preds, average='weighted')),\n",
    "        'per_class_report': classification_report(y_test_np, preds, \n",
    "                                                  target_names=star_names, \n",
    "                                                  output_dict=True)\n",
    "    }\n",
    "    detailed_results['models'].append(model_results)\n",
    "\n",
    "with open('../Outputs/dl_detailed_results.json', 'w') as f:\n",
    "    json.dump(detailed_results, f, indent=2)\n",
    "print(\"✓ Saved: dl_detailed_results.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL RESULTS SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ],
   "id": "d77d78782a58b9e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ],
   "id": "43dba1db51d1be02"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Final summary\n",
    "print(\"=\" * 80)\n",
    "print(\"DEEP LEARNING MODELS - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "OBJECTIVE:\n",
    "Train and evaluate deep learning models (RNN, LSTM, CNN-LSTM) for star rating prediction\n",
    "\n",
    "DATASET:\n",
    "- Training samples: {len(train_df):,}\n",
    "- Validation samples: {len(val_df):,}\n",
    "- Test samples: {len(test_df):,}\n",
    "- Vocabulary size: {vocab_size:,}\n",
    "- Max sequence length: {MAX_LEN}\n",
    "\n",
    "MODELS TRAINED:\n",
    "1. Basic RNN: {rnn_preds.shape[0]:,} parameters\n",
    "   → Test Accuracy: {accuracy_score(y_test_np, rnn_preds):.2%}\n",
    "   \n",
    "2. Bidirectional LSTM: {count_parameters(lstm_model):,} parameters\n",
    "   → Test Accuracy: {accuracy_score(y_test_np, lstm_preds):.2%}\n",
    "   \n",
    "3. Hybrid CNN-LSTM: {count_parameters(cnn_lstm_model):,} parameters\n",
    "   → Test Accuracy: {accuracy_score(y_test_np, cnn_lstm_preds):.2%}\n",
    "\n",
    "BEST MODEL: {best_model_name}\n",
    "- Accuracy: {best_accuracy:.2%}\n",
    "- F1 (Macro): {results_df.loc[best_idx, 'F1 (Macro)']:.4f}\n",
    "\n",
    "TRAINING DETAILS:\n",
    "- Device: {device}\n",
    "- Epochs: {N_EPOCHS}\n",
    "- Batch size: {BATCH_SIZE}\n",
    "- Optimizer: Adam (lr={LEARNING_RATE})\n",
    "- Loss: CrossEntropyLoss with class weights\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. Deep learning models show improvement over traditional ML\n",
    "2. {best_model_name} performs best\n",
    "3. Bidirectional LSTM captures better context than vanilla RNN\n",
    "4. CNN-LSTM hybrid combines local and sequential features effectively\n",
    "\n",
    "OUTPUTS SAVED:\n",
    "- Models: ../Outputs/Models/*.pt\n",
    "- Vocabulary: ../Outputs/Models/vocab.pkl\n",
    "- Plots: ../Outputs/Plots/*.png\n",
    "- Results: ../Outputs/*.csv, *.json\n",
    "\n",
    "NEXT STEPS:\n",
    "□ Compare with baseline models (RF, GBM)\n",
    "□ Ensemble predictions from multiple models\n",
    "□ Error analysis on deep learning predictions\n",
    "□ Try pre-trained embeddings (GloVe, Word2Vec)\n",
    "□ Experiment with attention mechanisms\n",
    "□ Fine-tune transformer models (DistilBERT, RoBERTa)\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open('../Outputs/dl_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n✓ Summary saved: dl_summary.txt\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 DEEP LEARNING TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ],
   "id": "b230e3431e7399cd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
