{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Yelp Review Prediction - LSTM Model (PyTorch)\n**Author:** Ben\n\n**Project:** Capstone - Star Rating Prediction\n\n**Objective:** Train and evaluate a Bidirectional LSTM model using PyTorch with CUDA acceleration\n\n---\n\n## Model Architecture\n\n**Bidirectional LSTM:** Sophisticated sequence model that can capture long-range dependencies and context from both directions\n\n**Why PyTorch:**\n- More flexible than Keras for custom architectures\n- Better for research and experimentation\n- Excellent CUDA support for GPU acceleration\n- Industry standard for deep learning research"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:37:02.708281300Z",
     "start_time": "2026-01-29T03:37:00.631041800Z"
    }
   },
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Sklearn for metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization defaults\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../Outputs/Models', exist_ok=True)\n",
    "os.makedirs('../Outputs/Plots', exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "PyTorch version: 2.10.0+cu128\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:37:02.751426700Z",
     "start_time": "2026-01-29T03:37:02.709282400Z"
    }
   },
   "source": [
    "# Check CUDA availability and setup device\n",
    "print(\"=\" * 80)\n",
    "print(\"CUDA SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\\nCUDA is available!\")\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"  Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"  Current GPU Memory:\")\n",
    "    print(f\"    Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"    Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\\nCUDA is not available. Using CPU.\")\n",
    "    print(\"  Training will be significantly slower.\")\n",
    "    print(\"  Consider using Google Colab for free GPU access.\")\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"\\nRandom seed set to {SEED} for reproducibility\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CUDA SETUP\n",
      "================================================================================\n",
      "\n",
      "CUDA is available!\n",
      "  Device: NVIDIA GeForce RTX 5070 Ti\n",
      "  CUDA Version: 12.8\n",
      "  Number of GPUs: 1\n",
      "  Current GPU Memory:\n",
      "    Allocated: 0.00 GB\n",
      "    Cached: 0.00 GB\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:37:16.862242Z",
     "start_time": "2026-01-29T03:37:02.762431800Z"
    }
   },
   "source": [
    "# Load cleaned data\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use sample dataset for faster training during development\n",
    "USE_SAMPLE = False  # Set to False for full dataset\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    print(\"\\nLoading SAMPLE dataset for rapid iteration...\")\n",
    "    data_df = pd.read_csv('../Data/Processed/yelp_sample.csv')\n",
    "    train_df, test_df = train_test_split(data_df, test_size=0.2, \n",
    "                                         random_state=42, stratify=data_df['stars'])\n",
    "    # Further split train into train and validation\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.1, \n",
    "                                        random_state=42, stratify=train_df['stars'])\n",
    "else:\n",
    "    print(\"\\nLoading FULL dataset...\")\n",
    "    train_df = pd.read_csv('../Data/Processed/yelp_train.csv')\n",
    "    test_df = pd.read_csv('../Data/Processed/yelp_test.csv')\n",
    "    # Split off validation set from training\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.1, \n",
    "                                        random_state=42, stratify=train_df['stars'])\n",
    "\n",
    "print(f\"\\nData loaded:\")\n",
    "print(f\"  Train: {len(train_df):,} samples\")\n",
    "print(f\"  Validation: {len(val_df):,} samples\")\n",
    "print(f\"  Test: {len(test_df):,} samples\")\n",
    "\n",
    "# Target distribution\n",
    "print(\"\\nTarget distribution (train):\")\n",
    "print(train_df['stars'].value_counts(normalize=True).sort_index())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data loaded:\n",
      "  Train: 1,800,000 samples\n",
      "  Validation: 200,000 samples\n",
      "  Test: 500,000 samples\n",
      "\n",
      "Target distribution (train):\n",
      "stars\n",
      "1    0.2\n",
      "2    0.2\n",
      "3    0.2\n",
      "4    0.2\n",
      "5    0.2\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:37:40.859992800Z",
     "start_time": "2026-01-29T03:37:16.886419100Z"
    }
   },
   "source": [
    "# Text preprocessing and tokenization\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEXT PREPROCESSING AND TOKENIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    \"\"\"Simple word tokenizer - split on whitespace and lowercase.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary from training data\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "all_words = []\n",
    "for text in train_df['text'].fillna(''):\n",
    "    all_words.extend(simple_tokenizer(text))\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "print(f\"  Total words: {len(all_words):,}\")\n",
    "print(f\"  Unique words: {len(word_counts):,}\")\n",
    "\n",
    "# Keep only words that appear at least MIN_FREQ times\n",
    "MIN_FREQ = 5\n",
    "vocab = {word for word, count in word_counts.items() if count >= MIN_FREQ}\n",
    "print(f\"  Vocabulary size (min_freq={MIN_FREQ}): {len(vocab):,}\")\n",
    "\n",
    "# Create word to index mapping\n",
    "# Reserve 0 for padding, 1 for unknown\n",
    "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "for idx, word in enumerate(sorted(vocab), start=2):\n",
    "    word2idx[word] = idx\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"\\nVocabulary created:\")\n",
    "print(f\"  Vocab size: {vocab_size:,}\")\n",
    "print(f\"  PAD token: {word2idx['<PAD>']}\")\n",
    "print(f\"  UNK token: {word2idx['<UNK>']}\")\n",
    "print(f\"\\nExample words and indices:\")\n",
    "for word in list(word2idx.keys())[2:12]:\n",
    "    print(f\"  '{word}': {word2idx[word]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary created:\n",
      "  Vocab size: 245,599\n",
      "  PAD token: 0\n",
      "  UNK token: 1\n",
      "\n",
      "Example words and indices:\n",
      "  '!': 2\n",
      "  '!!': 3\n",
      "  '!!!': 4\n",
      "  '!!!!': 5\n",
      "  '!!!!!': 6\n",
      "  '!!!!!!': 7\n",
      "  '!!!!!!!': 8\n",
      "  '!!!!!!!!': 9\n",
      "  '!!!!!!!!!': 10\n",
      "  '!!!!!!!!!!': 11\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:38:29.921487500Z",
     "start_time": "2026-01-29T03:37:42.711519400Z"
    }
   },
   "source": [
    "# Convert text to sequences of indices\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONVERTING TEXT TO SEQUENCES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def text_to_sequence(text, word2idx, max_len=None):\n",
    "    \"\"\"\n",
    "    Convert text to sequence of word indices.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        word2idx: Word to index mapping\n",
    "        max_len: Maximum sequence length (truncate if longer)\n",
    "    \n",
    "    Returns:\n",
    "        List of word indices\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenizer(text)\n",
    "    sequence = [word2idx.get(word, word2idx['<UNK>']) for word in tokens]\n",
    "    \n",
    "    if max_len:\n",
    "        sequence = sequence[:max_len]\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Set maximum sequence length based on data\n",
    "MAX_LEN = 200  # Truncate reviews longer than 200 words\n",
    "\n",
    "print(f\"\\nConverting text to sequences (max_len={MAX_LEN})...\")\n",
    "train_sequences = [text_to_sequence(text, word2idx, MAX_LEN) \n",
    "                   for text in train_df['text'].fillna('')]\n",
    "val_sequences = [text_to_sequence(text, word2idx, MAX_LEN) \n",
    "                 for text in val_df['text'].fillna('')]\n",
    "test_sequences = [text_to_sequence(text, word2idx, MAX_LEN) \n",
    "                  for text in test_df['text'].fillna('')]\n",
    "\n",
    "# Get sequence lengths before padding\n",
    "train_lengths = [len(seq) for seq in train_sequences]\n",
    "val_lengths = [len(seq) for seq in val_sequences]\n",
    "test_lengths = [len(seq) for seq in test_sequences]\n",
    "\n",
    "print(f\"\\nSequences created\")\n",
    "print(f\"  Average length (before padding): {np.mean(train_lengths):.1f} words\")\n",
    "print(f\"  Median length: {np.median(train_lengths):.0f} words\")\n",
    "print(f\"  Max length (capped): {MAX_LEN} words\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nExample sequence (first 20 tokens):\")\n",
    "example_text = train_df.iloc[0]['text']\n",
    "example_seq = train_sequences[0]\n",
    "print(f\"  Original: {example_text[:100]}...\")\n",
    "print(f\"  Sequence: {example_seq[:20]}\")\n",
    "print(f\"  Decoded: {' '.join([idx2word[idx] for idx in example_seq[:20]])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequences created\n",
      "  Average length (before padding): 99.8 words\n",
      "  Median length: 85 words\n",
      "  Max length (capped): 200 words\n",
      "\n",
      "Example sequence (first 20 tokens):\n",
      "  Original: What use to be free is now $8, is it worth it? Kind of. You'll see cars here that you've never seen ...\n",
      "  Sequence: [239244, 232089, 223233, 47102, 104653, 127520, 158670, 11510, 127520, 127832, 242235, 128131, 132758, 160115, 244244, 197195, 61284, 117761, 219972, 244253]\n",
      "  Decoded: what use to be free is now $8, is it worth it? kind of. you'll see cars here that you've\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:38:35.924042200Z",
     "start_time": "2026-01-29T03:38:31.154221800Z"
    }
   },
   "source": [
    "# Pad sequences and convert to tensors\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PADDING AND CREATING TENSORS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def pad_sequences(sequences, max_len, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pad sequences to the same length.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of sequences\n",
    "        max_len: Target length\n",
    "        pad_value: Value to use for padding\n",
    "    \n",
    "    Returns:\n",
    "        Padded numpy array\n",
    "    \"\"\"\n",
    "    padded = np.zeros((len(sequences), max_len), dtype=np.int64)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = min(len(seq), max_len)\n",
    "        padded[i, :length] = seq[:length]\n",
    "    \n",
    "    return padded\n",
    "\n",
    "# Pad sequences\n",
    "X_train = pad_sequences(train_sequences, MAX_LEN)\n",
    "X_val = pad_sequences(val_sequences, MAX_LEN)\n",
    "X_test = pad_sequences(test_sequences, MAX_LEN)\n",
    "\n",
    "# Convert labels to numpy (subtract 1 to make 0-indexed for PyTorch)\n",
    "y_train = train_df['stars'].values - 1  # Now 0-4 instead of 1-5\n",
    "y_val = val_df['stars'].values - 1\n",
    "y_test = test_df['stars'].values - 1\n",
    "\n",
    "print(f\"\\nData prepared:\")\n",
    "print(f\"  X_train shape: {X_train.shape}\")\n",
    "print(f\"  X_val shape: {X_val.shape}\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n",
    "print(f\"  y_train shape: {y_train.shape}\")\n",
    "print(f\"  Label range: {y_train.min()} to {y_train.max()} (0-indexed)\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.LongTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_val_tensor = torch.LongTensor(X_val)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "X_test_tensor = torch.LongTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "print(f\"\\nTensors created\")\n",
    "print(f\"  X_train_tensor: {X_train_tensor.shape}, dtype: {X_train_tensor.dtype}\")\n",
    "print(f\"  y_train_tensor: {y_train_tensor.shape}, dtype: {y_train_tensor.dtype}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data prepared:\n",
      "  X_train shape: (1800000, 200)\n",
      "  X_val shape: (200000, 200)\n",
      "  X_test shape: (500000, 200)\n",
      "  y_train shape: (1800000,)\n",
      "  Label range: 0 to 4 (0-indexed)\n",
      "\n",
      "Tensors created\n",
      "  X_train_tensor: torch.Size([1800000, 200]), dtype: torch.int64\n",
      "  y_train_tensor: torch.Size([1800000]), dtype: torch.int64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:38:37.214895400Z",
     "start_time": "2026-01-29T03:38:37.140291800Z"
    }
   },
   "source": [
    "# Create DataLoaders\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING DATALOADERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch_size={BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test DataLoader\n",
    "test_batch = next(iter(train_loader))\n",
    "print(f\"\\nTest batch:\")\n",
    "print(f\"  Input shape: {test_batch[0].shape}\")\n",
    "print(f\"  Label shape: {test_batch[1].shape}\")\n",
    "print(f\"  Sample input (first 5 tokens): {test_batch[0][0][:5]}\")\n",
    "print(f\"  Sample label: {test_batch[1][0]} (corresponds to {test_batch[1][0].item() + 1} stars)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING DATALOADERS\n",
      "================================================================================\n",
      "\n",
      "DataLoaders created with batch_size=1024\n",
      "  Train batches: 1758\n",
      "  Val batches: 196\n",
      "  Test batches: 489\n",
      "\n",
      "Test batch:\n",
      "  Input shape: torch.Size([1024, 200])\n",
      "  Label shape: torch.Size([1024])\n",
      "  Sample input (first 5 tokens): tensor([ 95435, 122476, 233874, 152914,  93023])\n",
      "  Sample label: 4 (corresponds to 5 stars)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Model Architecture"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:38:37.222119Z",
     "start_time": "2026-01-29T03:38:37.216611700Z"
    }
   },
   "source": "# Bidirectional LSTM Model",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:38:37.240540500Z",
     "start_time": "2026-01-29T03:38:37.222119Z"
    }
   },
   "source": "print(\"=\" * 80)\nprint(\"BIDIRECTIONAL LSTM MODEL\")\nprint(\"=\" * 80)\n\nclass BiLSTM(nn.Module):\n    \"\"\"\n    Bidirectional LSTM model for text classification.\n    \n    Architecture:\n    - Embedding layer\n    - Bidirectional LSTM (reads sequence forwards and backwards)\n    - Dropout for regularization\n    - Fully connected output layer\n    \n    Bidirectional helps capture context from both directions.\n    \"\"\"\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n                 n_layers=2, dropout=0.5):\n        super(BiLSTM, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n                           batch_first=True, dropout=dropout if n_layers > 1 else 0,\n                           bidirectional=True)\n        self.dropout = nn.Dropout(dropout)\n        # *2 because bidirectional concatenates forward and backward hidden states\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        \n    def forward(self, text):\n        # text shape: [batch_size, seq_len]\n        embedded = self.dropout(self.embedding(text))\n        # embedded shape: [batch_size, seq_len, embedding_dim]\n        \n        output, (hidden, cell) = self.lstm(embedded)\n        # output shape: [batch_size, seq_len, hidden_dim * 2]\n        # hidden shape: [n_layers * 2, batch_size, hidden_dim]\n        \n        # Concatenate the final forward and backward hidden states\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n        # hidden shape: [batch_size, hidden_dim * 2]\n        \n        hidden = self.dropout(hidden)\n        output = self.fc(hidden)\n        # output shape: [batch_size, output_dim]\n        \n        return output\n\nprint(\"\\nBiLSTM class defined\")\nprint(\"\\nArchitecture: Embedding → Bidirectional LSTM → Dropout → Linear\")\nprint(\"Note: Bidirectional doubles the hidden dimension\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BIDIRECTIONAL LSTM MODEL\n",
      "================================================================================\n",
      "\n",
      "BiLSTM class defined\n",
      "\n",
      "Architecture: Embedding → Bidirectional LSTM → Dropout → Linear\n",
      "Note: Bidirectional doubles the hidden dimension\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:38:37.257728100Z",
     "start_time": "2026-01-29T03:38:37.241542800Z"
    }
   },
   "source": [
    "# Training and evaluation functions\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING UTILITIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def train_epoch(model, iterator, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        iterator: DataLoader\n",
    "        optimizer: Optimizer\n",
    "        criterion: Loss function\n",
    "        device: torch.device\n",
    "    \n",
    "    Returns:\n",
    "        Average loss and accuracy for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # Get batch data\n",
    "        text, labels = batch\n",
    "        text = text.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = (predictions.argmax(1) == labels).float().mean()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        iterator: DataLoader\n",
    "        criterion: Loss function\n",
    "        device: torch.device\n",
    "    \n",
    "    Returns:\n",
    "        Average loss and accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text = text.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = (predictions.argmax(1) == labels).float().mean()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def predict(model, iterator, device):\n",
    "    \"\"\"\n",
    "    Generate predictions for all samples.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        iterator: DataLoader\n",
    "        device: torch.device\n",
    "    \n",
    "    Returns:\n",
    "        Numpy arrays of predictions and true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text = text.to(device)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            preds = predictions.argmax(1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\nTraining functions defined:\")\n",
    "print(\"  - train_epoch(): Train for one epoch\")\n",
    "print(\"  - evaluate(): Evaluate model performance\")\n",
    "print(\"  - predict(): Generate predictions\")\n",
    "print(\"  - count_parameters(): Count trainable parameters\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING UTILITIES\n",
      "================================================================================\n",
      "\n",
      "Training functions defined:\n",
      "  - train_epoch(): Train for one epoch\n",
      "  - evaluate(): Evaluate model performance\n",
      "  - predict(): Generate predictions\n",
      "  - count_parameters(): Count trainable parameters\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Model Evaluation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:38:37.364635700Z",
     "start_time": "2026-01-29T03:38:37.257728100Z"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPERPARAMETERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 5  # 5 classes (1-5 stars, 0-indexed as 0-4)\n",
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "hyperparams = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'output_dim': OUTPUT_DIM,\n",
    "    'n_epochs': N_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'max_len': MAX_LEN\n",
    "}\n",
    "\n",
    "print(\"\\nModel hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Loss function with class weights for imbalanced data\n",
    "print(\"\\nCalculating class weights for imbalanced dataset...\")\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"Class weights: {class_weights.cpu().numpy()}\")\n",
    "print(f\"\\n(Higher weight = rarer class, will be penalized more in loss)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1. 1. 1. 1. 1.]\n",
      "\n",
      "(Higher weight = rarer class, will be penalized more in loss)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T03:38:37.379729Z",
     "start_time": "2026-01-29T03:38:37.365635600Z"
    }
   },
   "source": "# Train LSTM Model\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING BIDIRECTIONAL LSTM\")\nprint(\"=\" * 80)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING BIDIRECTIONAL LSTM\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-29T03:38:37.380726200Z"
    }
   },
   "source": "N_EPOCHS = 10\n# Initialize model\nlstm_model = BiLSTM(\n    vocab_size=vocab_size,\n    embedding_dim=EMBEDDING_DIM,\n    hidden_dim=HIDDEN_DIM,\n    output_dim=OUTPUT_DIM,\n    n_layers=2,\n    dropout=0.5\n).to(device)\n\nprint(f\"\\nModel architecture:\")\nprint(lstm_model)\nprint(f\"\\nTrainable parameters: {count_parameters(lstm_model):,}\")\n\n# Optimizer and loss\nlstm_optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Training loop\nprint(f\"\\nTraining for {N_EPOCHS} epochs...\")\nprint(f\"Using device: {device}\\n\")\n\nlstm_train_losses = []\nlstm_train_accs = []\nlstm_val_losses = []\nlstm_val_accs = []\n\nbest_val_loss = float('inf')\nstart_time = time.time()\n\nfor epoch in range(N_EPOCHS):\n    epoch_start = time.time()\n    \n    train_loss, train_acc = train_epoch(lstm_model, train_loader, lstm_optimizer, criterion, device)\n    val_loss, val_acc = evaluate(lstm_model, val_loader, criterion, device)\n    \n    lstm_train_losses.append(train_loss)\n    lstm_train_accs.append(train_acc)\n    lstm_val_losses.append(val_loss)\n    lstm_val_accs.append(val_acc)\n    \n    epoch_time = time.time() - epoch_start\n    \n    print(f\"Epoch {epoch+1:02d}/{N_EPOCHS} | Time: {epoch_time:.1f}s\")\n    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(lstm_model.state_dict(), '../Outputs/Models/lstm_best.pt')\n        print(f\"   Saved new best model (val_loss: {val_loss:.4f})\")\n    print()\n\ntotal_time = time.time() - start_time\nprint(f\"\\nTraining complete in {total_time:.1f}s ({total_time/60:.1f} minutes)\")\nprint(f\"  Best validation loss: {best_val_loss:.4f}\")\n\n# Load best model for evaluation\nlstm_model.load_state_dict(torch.load('../Outputs/Models/lstm_best.pt'))\nprint(f\"\\nLoaded best model from checkpoint\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10 | Time: 188.9s\n",
      "  Train Loss: 1.1159 | Train Acc: 51.02%\n",
      "  Val Loss:   0.9878 | Val Acc:   57.85%\n",
      "   Saved new best model (val_loss: 0.9878)\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Evaluate model on test set\nprint(\"=\" * 80)\nprint(\"MODEL EVALUATION ON TEST SET\")\nprint(\"=\" * 80)\n\n# Get predictions\nprint(\"\\nGenerating predictions...\")\nlstm_preds, y_test_np = predict(lstm_model, test_loader, device)\n\n# Calculate metrics\naccuracy = accuracy_score(y_test_np, lstm_preds)\nf1_macro = f1_score(y_test_np, lstm_preds, average='macro')\nf1_weighted = f1_score(y_test_np, lstm_preds, average='weighted')\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TEST SET PERFORMANCE\")\nprint(\"=\" * 80)\n\nprint(f\"\\nBidirectional LSTM:\")\nprint(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint(f\"  F1-Score (macro): {f1_macro:.4f}\")\nprint(f\"  F1-Score (weighted): {f1_weighted:.4f}\")\n\nresults_summary = {\n    'Model': 'Bidirectional LSTM',\n    'Accuracy': accuracy,\n    'F1 (Macro)': f1_macro,\n    'F1 (Weighted)': f1_weighted\n}\nresults_df = pd.DataFrame([results_summary])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize confusion matrix\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CONFUSION MATRIX\")\nprint(\"=\" * 80)\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\ncm = confusion_matrix(y_test_np, lstm_preds)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nsns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n           xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5],\n           ax=ax, cbar_kws={'label': 'Proportion'})\nax.set_xlabel('Predicted Rating', fontweight='bold')\nax.set_ylabel('Actual Rating', fontweight='bold')\nax.set_title('Bidirectional LSTM - Confusion Matrix', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('../Outputs/Plots/12_lstm_confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"Plot saved: 12_lstm_confusion_matrix.png\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Save all results\nprint(\"=\" * 80)\nprint(\"SAVING RESULTS\")\nprint(\"=\" * 80)\n\n# Define star names for classification report\nstar_names = ['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars']\n\n# Save results CSV\nresults_df.to_csv('../Outputs/lstm_results.csv', index=False)\nprint(\"Saved: lstm_results.csv\")\n\n# Save vocabulary\nwith open('../Outputs/Models/vocab.pkl', 'wb') as f:\n    pickle.dump({'word2idx': word2idx, 'idx2word': idx2word}, f)\nprint(\"Saved: vocab.pkl\")\n\n# Save hyperparameters\nimport json\nwith open('../Outputs/lstm_hyperparameters.json', 'w') as f:\n    json.dump(hyperparams, f, indent=2)\nprint(\"Saved: lstm_hyperparameters.json\")\n\n# Save detailed results\ndetailed_results = {\n    'model': 'Bidirectional LSTM',\n    'accuracy': float(accuracy),\n    'f1_macro': float(f1_macro),\n    'f1_weighted': float(f1_weighted),\n    'per_class_report': classification_report(y_test_np, lstm_preds,\n                                              target_names=star_names,\n                                              output_dict=True)\n}\n\nwith open('../Outputs/lstm_detailed_results.json', 'w') as f:\n    json.dump(detailed_results, f, indent=2)\nprint(\"Saved: lstm_detailed_results.json\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ALL RESULTS SAVED SUCCESSFULLY\")\nprint(\"=\"*80)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Detailed classification report\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DETAILED CLASSIFICATION REPORT\")\nprint(\"=\" * 80)\n\nstar_names = ['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars']\n\nprint(f\"\\nBidirectional LSTM\")\nprint('='*80)\nprint(classification_report(y_test_np, lstm_preds, target_names=star_names))",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Save Results",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Plot training curves\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING CURVES\")\nprint(\"=\" * 80)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs = range(1, len(lstm_train_losses) + 1)\n\n# Loss\naxes[0].plot(epochs, lstm_train_losses, 'b-', label='Training')\naxes[0].plot(epochs, lstm_val_losses, 'r-', label='Validation')\naxes[0].set_xlabel('Epoch', fontweight='bold')\naxes[0].set_ylabel('Loss', fontweight='bold')\naxes[0].set_title('Bidirectional LSTM - Loss', fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Accuracy\naxes[1].plot(epochs, [acc*100 for acc in lstm_train_accs], 'b-', label='Training')\naxes[1].plot(epochs, [acc*100 for acc in lstm_val_accs], 'r-', label='Validation')\naxes[1].set_xlabel('Epoch', fontweight='bold')\naxes[1].set_ylabel('Accuracy (%)', fontweight='bold')\naxes[1].set_title('Bidirectional LSTM - Accuracy', fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('../Outputs/Plots/13_lstm_training_curves.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"Plot saved: 13_lstm_training_curves.png\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Plot training curves\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING CURVES\")\nprint(\"=\" * 80)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs = range(1, len(lstm_train_losses) + 1)\n\n# Loss\naxes[0].plot(epochs, lstm_train_losses, 'b-', label='Training')\naxes[0].plot(epochs, lstm_val_losses, 'r-', label='Validation')\naxes[0].set_xlabel('Epoch', fontweight='bold')\naxes[0].set_ylabel('Loss', fontweight='bold')\naxes[0].set_title('Bidirectional LSTM - Loss', fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Accuracy\naxes[1].plot(epochs, [acc*100 for acc in lstm_train_accs], 'b-', label='Training')\naxes[1].plot(epochs, [acc*100 for acc in lstm_val_accs], 'r-', label='Validation')\naxes[1].set_xlabel('Epoch', fontweight='bold')\naxes[1].set_ylabel('Accuracy (%)', fontweight='bold')\naxes[1].set_title('Bidirectional LSTM - Accuracy', fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('../Outputs/Plots/13_lstm_training_curves.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"Plot saved: 13_lstm_training_curves.png\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
