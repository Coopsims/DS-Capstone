{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Yelp Review Prediction - Baseline Models\n**Author:** Ben\n\n**Project:** Capstone - Star Rating Prediction\n\n**Objective:** Train and evaluate baseline models (Random Forest and XGBoost) to predict star ratings from review text and metadata\n\n---\n\n## Model Approach\n\n**Research Question:** How do traditional ML models (Random Forest, XGBoost) perform on star rating prediction when combining text features with business metadata?\n\n**Models Selected:**\n1. **Random Forest**: Ensemble method robust to overfitting, handles non-linear relationships\n2. **XGBoost**: High-performance gradient boosting that natively supports sparse matrices and parallelization\n\n**Features:**\n- Text: TF-IDF vectors (top 5000 terms)\n- Metadata: Business category, location, review length, engagement metrics, temporal features\n\n**Evaluation:**\n- Accuracy, Precision, Recall, F1-score (per class and macro average)\n- Confusion matrices\n- Feature importance analysis\n- Error analysis"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:41:05.841693600Z",
     "start_time": "2026-01-22T03:41:05.622339300Z"
    }
   },
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy import sparse\n",
    "\n",
    "# XGBoost import\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization defaults\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../Outputs/Models', exist_ok=True)\n",
    "os.makedirs('../Outputs/Plots', exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Current time: 2026-01-21 20:41:05\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:41:18.997861900Z",
     "start_time": "2026-01-22T03:41:05.852692300Z"
    }
   },
   "source": [
    "# Load cleaned data\n",
    "print(\"Loading cleaned datasets...\\n\")\n",
    "\n",
    "# Start with sample for rapid prototyping\n",
    "USE_SAMPLE = False  # Set to False to use full dataset\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    print(\"Loading SAMPLE dataset (50k rows) for rapid iteration...\")\n",
    "    train_df = pd.read_csv('../Data/Processed/yelp_sample.csv')\n",
    "    # Create a small test set from sample\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(train_df, test_size=0.2, \n",
    "                                         random_state=42, stratify=train_df['stars'])\n",
    "    print(f\"Sample loaded: {len(train_df):,} train, {len(test_df):,} test\")\n",
    "else:\n",
    "    print(\"Loading FULL dataset...\")\n",
    "    train_df = pd.read_csv('../Data/Processed/yelp_train.csv')\n",
    "    test_df = pd.read_csv('../Data/Processed/yelp_test.csv')\n",
    "    print(f\"Full data loaded: {len(train_df):,} train, {len(test_df):,} test\")\n",
    "\n",
    "print(f\"\\nTrain shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Verify target distribution\n",
    "print(\"\\nTarget distribution (train):\")\n",
    "print(train_df['stars'].value_counts(normalize=True).sort_index())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned datasets...\n",
      "\n",
      "Loading FULL dataset...\n",
      "Full data loaded: 2,000,000 train, 500,000 test\n",
      "\n",
      "Train shape: (2000000, 25)\n",
      "Test shape: (500000, 25)\n",
      "\n",
      "Target distribution (train):\n",
      "stars\n",
      "1    0.2\n",
      "2    0.2\n",
      "3    0.2\n",
      "4    0.2\n",
      "5    0.2\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:41:19.067271200Z",
     "start_time": "2026-01-22T03:41:19.042333500Z"
    }
   },
   "source": [
    "# Extract target variable\n",
    "y_train = train_df['stars'].values\n",
    "y_test = test_df['stars'].values\n",
    "\n",
    "print(f\"Target variable extracted:\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable extracted:\n",
      "y_train shape: (2000000,)\n",
      "y_test shape: (500000,)\n",
      "Classes: [1 2 3 4 5]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T03:42:12.479362300Z",
     "start_time": "2026-01-22T03:41:19.068271Z"
    }
   },
   "source": [
    "# Text Feature Extraction - TF-IDF\n",
    "print(\"=\" * 80)\n",
    "print(\"TEXT FEATURE EXTRACTION (TF-IDF)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nFitting TF-IDF vectorizer on training data...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=7000,      # Limit to top 5000 terms\n",
    "    min_df=5,               # Term must appear in at least 5 documents\n",
    "    max_df=0.85,             # Ignore terms in more than 80% of documents\n",
    "    ngram_range=(1, 2),     # Use unigrams and bigrams\n",
    "    stop_words='english',   # Remove common English stop words\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train_text = tfidf.fit_transform(train_df['text'].fillna(''))\n",
    "X_test_text = tfidf.transform(test_df['text'].fillna(''))\n",
    "\n",
    "elapsed = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"TF-IDF completed in {elapsed:.1f} seconds\")\n",
    "print(f\"\\nTF-IDF feature dimensions:\")\n",
    "print(f\"Train: {X_train_text.shape}\")\n",
    "print(f\"Test: {X_test_text.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"Sparsity: {(1.0 - X_train_text.nnz / (X_train_text.shape[0] * X_train_text.shape[1])):.1%}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEXT FEATURE EXTRACTION (TF-IDF)\n",
      "================================================================================\n",
      "\n",
      "Fitting TF-IDF vectorizer on training data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m      9\u001B[39m tfidf = TfidfVectorizer(\n\u001B[32m     10\u001B[39m     max_features=\u001B[32m7000\u001B[39m,      \u001B[38;5;66;03m# Limit to top 5000 terms\u001B[39;00m\n\u001B[32m     11\u001B[39m     min_df=\u001B[32m5\u001B[39m,               \u001B[38;5;66;03m# Term must appear in at least 5 documents\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     16\u001B[39m     strip_accents=\u001B[33m'\u001B[39m\u001B[33municode\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m     17\u001B[39m )\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# Fit on training data and transform both train and test\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m X_train_text = \u001B[43mtfidf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtext\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfillna\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m X_test_text = tfidf.transform(test_df[\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m].fillna(\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m     23\u001B[39m elapsed = (datetime.now() - start_time).total_seconds()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DS-Capstone\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001B[39m, in \u001B[36mTfidfVectorizer.fit_transform\u001B[39m\u001B[34m(self, raw_documents, y)\u001B[39m\n\u001B[32m   2097\u001B[39m \u001B[38;5;28mself\u001B[39m._check_params()\n\u001B[32m   2098\u001B[39m \u001B[38;5;28mself\u001B[39m._tfidf = TfidfTransformer(\n\u001B[32m   2099\u001B[39m     norm=\u001B[38;5;28mself\u001B[39m.norm,\n\u001B[32m   2100\u001B[39m     use_idf=\u001B[38;5;28mself\u001B[39m.use_idf,\n\u001B[32m   2101\u001B[39m     smooth_idf=\u001B[38;5;28mself\u001B[39m.smooth_idf,\n\u001B[32m   2102\u001B[39m     sublinear_tf=\u001B[38;5;28mself\u001B[39m.sublinear_tf,\n\u001B[32m   2103\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m2104\u001B[39m X = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2105\u001B[39m \u001B[38;5;28mself\u001B[39m._tfidf.fit(X)\n\u001B[32m   2106\u001B[39m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[32m   2107\u001B[39m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DS-Capstone\\.venv\\Lib\\site-packages\\sklearn\\base.py:1336\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1329\u001B[39m     estimator._validate_params()\n\u001B[32m   1331\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1332\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1333\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1334\u001B[39m     )\n\u001B[32m   1335\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1336\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DS-Capstone\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1386\u001B[39m, in \u001B[36mCountVectorizer.fit_transform\u001B[39m\u001B[34m(self, raw_documents, y)\u001B[39m\n\u001B[32m   1378\u001B[39m             warnings.warn(\n\u001B[32m   1379\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mUpper case characters found in\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1380\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m vocabulary while \u001B[39m\u001B[33m'\u001B[39m\u001B[33mlowercase\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1381\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m is True. These entries will not\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1382\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m be matched with any documents\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1383\u001B[39m             )\n\u001B[32m   1384\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1386\u001B[39m vocabulary, X = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1388\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.binary:\n\u001B[32m   1389\u001B[39m     X.data.fill(\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DS-Capstone\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1273\u001B[39m, in \u001B[36mCountVectorizer._count_vocab\u001B[39m\u001B[34m(self, raw_documents, fixed_vocab)\u001B[39m\n\u001B[32m   1271\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[32m   1272\u001B[39m     feature_counter = {}\n\u001B[32m-> \u001B[39m\u001B[32m1273\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m   1274\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1275\u001B[39m             feature_idx = vocabulary[feature]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DS-Capstone\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:118\u001B[39m, in \u001B[36m_analyze\u001B[39m\u001B[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[39m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ngrams \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    117\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m stop_words \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m         doc = \u001B[43mngrams\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_words\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    120\u001B[39m         doc = ngrams(doc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DS-Capstone\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:251\u001B[39m, in \u001B[36m_VectorizerMixin._word_ngrams\u001B[39m\u001B[34m(self, tokens, stop_words)\u001B[39m\n\u001B[32m    249\u001B[39m \u001B[38;5;66;03m# handle stop words\u001B[39;00m\n\u001B[32m    250\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stop_words \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m251\u001B[39m     tokens = [w \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m tokens \u001B[38;5;28;01mif\u001B[39;00m w \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words]\n\u001B[32m    253\u001B[39m \u001B[38;5;66;03m# handle token n-grams\u001B[39;00m\n\u001B[32m    254\u001B[39m min_n, max_n = \u001B[38;5;28mself\u001B[39m.ngram_range\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show top TF-IDF terms\n",
    "print(\"\\nTop 20 TF-IDF terms by average score:\")\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "avg_tfidf = np.asarray(X_train_text.mean(axis=0)).flatten()\n",
    "top_indices = avg_tfidf.argsort()[-20:][::-1]\n",
    "\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"{i:2d}. {feature_names[idx]:20s} (avg score: {avg_tfidf[idx]:.4f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Metadata Feature Engineering\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METADATA FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_metadata_features(df, encoders=None, fit=True):\n",
    "    \"\"\"\n",
    "    Extract and encode metadata features from the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        encoders: Dictionary of fitted encoders (for test set)\n",
    "        fit: Whether to fit encoders (True for train, False for test)\n",
    "    \n",
    "    Returns:\n",
    "        features: Numpy array of features\n",
    "        encoders: Dictionary of encoders (if fit=True)\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    feature_names = []\n",
    "    \n",
    "    if encoders is None:\n",
    "        encoders = {}\n",
    "    \n",
    "    # 1. Numerical features (already scaled by nature)\n",
    "    numerical_features = [\n",
    "        'word_count',\n",
    "        'char_count', \n",
    "        'useful',\n",
    "        'funny',\n",
    "        'cool',\n",
    "        'total_engagement',\n",
    "        'review_count',\n",
    "        'user_review_count'\n",
    "    ]\n",
    "    \n",
    "    for col in numerical_features:\n",
    "        if col in df.columns:\n",
    "            values = df[col].fillna(0).values.reshape(-1, 1)\n",
    "            \n",
    "            if fit:\n",
    "                scaler = StandardScaler()\n",
    "                scaled_values = scaler.fit_transform(values)\n",
    "                encoders[f'{col}_scaler'] = scaler\n",
    "            else:\n",
    "                scaled_values = encoders[f'{col}_scaler'].transform(values)\n",
    "            \n",
    "            features_list.append(scaled_values)\n",
    "            feature_names.append(col)\n",
    "    \n",
    "    # 2. Binary features\n",
    "    binary_features = ['is_open']\n",
    "    for col in binary_features:\n",
    "        if col in df.columns:\n",
    "            features_list.append(df[col].fillna(1).values.reshape(-1, 1))\n",
    "            feature_names.append(col)\n",
    "    \n",
    "    # 3. Temporal features\n",
    "    temporal_features = ['year', 'month', 'day_of_week']\n",
    "    for col in temporal_features:\n",
    "        if col in df.columns:\n",
    "            values = df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else 0).values.reshape(-1, 1)\n",
    "            features_list.append(values)\n",
    "            feature_names.append(col)\n",
    "    \n",
    "    # 4. Categorical features - State (top 10 only)\n",
    "    if 'state' in df.columns:\n",
    "        if fit:\n",
    "            top_states = df['state'].value_counts().head(10).index.tolist()\n",
    "            encoders['top_states'] = top_states\n",
    "        else:\n",
    "            top_states = encoders['top_states']\n",
    "        \n",
    "        # Create binary features for top states\n",
    "        for state in top_states:\n",
    "            features_list.append((df['state'] == state).astype(int).values.reshape(-1, 1))\n",
    "            feature_names.append(f'state_{state}')\n",
    "    \n",
    "    # 5. Categories - Extract top categories\n",
    "    if 'categories' in df.columns:\n",
    "        if fit:\n",
    "            # Find most common categories\n",
    "            all_cats = []\n",
    "            for cats in df['categories'].fillna('').str.split(', '):\n",
    "                all_cats.extend(cats)\n",
    "            from collections import Counter\n",
    "            top_categories = [cat for cat, _ in Counter(all_cats).most_common(20) if cat]\n",
    "            encoders['top_categories'] = top_categories\n",
    "        else:\n",
    "            top_categories = encoders['top_categories']\n",
    "        \n",
    "        # Create binary features for top categories\n",
    "        for category in top_categories:\n",
    "            has_category = df['categories'].fillna('').str.contains(category, case=False, regex=False)\n",
    "            features_list.append(has_category.astype(int).values.reshape(-1, 1))\n",
    "            feature_names.append(f'category_{category.replace(\" \", \"_\")}')\n",
    "    \n",
    "    # 6. User activity level\n",
    "    if 'user_activity' in df.columns:\n",
    "        activity_mapping = {\n",
    "            '1 review': 1,\n",
    "            '2-5 reviews': 2,\n",
    "            '6-20 reviews': 3,\n",
    "            '21-100 reviews': 4,\n",
    "            '100+ reviews': 5\n",
    "        }\n",
    "        activity_encoded = df['user_activity'].map(activity_mapping).fillna(1).values.reshape(-1, 1)\n",
    "        features_list.append(activity_encoded)\n",
    "        feature_names.append('user_activity_level')\n",
    "    \n",
    "    # Combine all features\n",
    "    X_metadata = np.hstack(features_list)\n",
    "    \n",
    "    if fit:\n",
    "        return X_metadata, encoders, feature_names\n",
    "    else:\n",
    "        return X_metadata, feature_names\n",
    "\n",
    "# Extract features\n",
    "print(\"\\nExtracting metadata features...\")\n",
    "X_train_meta, metadata_encoders, metadata_feature_names = extract_metadata_features(train_df, fit=True)\n",
    "X_test_meta, _ = extract_metadata_features(test_df, encoders=metadata_encoders, fit=False)\n",
    "\n",
    "print(f\"\\nMetadata features extracted\")\n",
    "print(f\"Train shape: {X_train_meta.shape}\")\n",
    "print(f\"Test shape: {X_test_meta.shape}\")\n",
    "print(f\"Number of metadata features: {len(metadata_feature_names)}\")\n",
    "print(f\"\\nMetadata feature names:\")\n",
    "for i, name in enumerate(metadata_feature_names[:20], 1):  # Show first 20\n",
    "    print(f\"{i:2d}. {name}\")\n",
    "if len(metadata_feature_names) > 20:\n",
    "    print(f\"... and {len(metadata_feature_names) - 20} more\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Combine text and metadata features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMBINING FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert metadata to sparse matrix for efficient storage\n",
    "X_train_meta_sparse = sparse.csr_matrix(X_train_meta)\n",
    "X_test_meta_sparse = sparse.csr_matrix(X_test_meta)\n",
    "\n",
    "# Combine text (TF-IDF) and metadata\n",
    "X_train_combined = sparse.hstack([X_train_text, X_train_meta_sparse])\n",
    "X_test_combined = sparse.hstack([X_test_text, X_test_meta_sparse])\n",
    "\n",
    "print(f\"\\nFeatures combined\")\n",
    "print(f\"Text features: {X_train_text.shape[1]}\")\n",
    "print(f\"Metadata features: {X_train_meta.shape[1]}\")\n",
    "print(f\"Total features: {X_train_combined.shape[1]}\")\n",
    "print(f\"\\nFinal feature matrices:\")\n",
    "print(f\"X_train shape: {X_train_combined.shape}\")\n",
    "print(f\"X_test shape: {X_test_combined.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model - Most Frequent Class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate baseline accuracy (always predict most frequent class)\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE MODEL - MOST FREQUENT CLASS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "most_frequent_class = pd.Series(y_train).value_counts().idxmax()\n",
    "baseline_predictions = np.full(len(y_test), most_frequent_class)\n",
    "baseline_accuracy = accuracy_score(y_test, baseline_predictions)\n",
    "\n",
    "print(f\"\\nMost frequent class: {int(most_frequent_class)} stars\")\n",
    "print(f\"Baseline accuracy (always predict {int(most_frequent_class)} stars): {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nThis is our minimum performance threshold - any model must beat {baseline_accuracy:.1%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train Random Forest - Text Only\n",
    "print(\"=\" * 80)\n",
    "print(\"RANDOM FOREST - TEXT ONLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTraining Random Forest on text features only...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "rf_text = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_text.fit(X_train_text, y_train)\n",
    "\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf_text = rf_text.predict(X_test_text)\n",
    "\n",
    "# Metrics\n",
    "rf_text_accuracy = accuracy_score(y_test, y_pred_rf_text)\n",
    "rf_text_f1_macro = f1_score(y_test, y_pred_rf_text, average='macro')\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"Accuracy: {rf_text_accuracy:.4f} ({rf_text_accuracy*100:.2f}%)\")\n",
    "print(f\"F1-Score (macro): {rf_text_f1_macro:.4f}\")\n",
    "print(f\"Improvement over baseline: +{(rf_text_accuracy - baseline_accuracy)*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train Random Forest - Text + Metadata\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANDOM FOREST - TEXT + METADATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTraining Random Forest on combined features (text + metadata)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "rf_combined = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_combined.fit(X_train_combined, y_train)\n",
    "\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf_combined = rf_combined.predict(X_test_combined)\n",
    "\n",
    "# Metrics\n",
    "rf_combined_accuracy = accuracy_score(y_test, y_pred_rf_combined)\n",
    "rf_combined_f1_macro = f1_score(y_test, y_pred_rf_combined, average='macro')\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"Accuracy: {rf_combined_accuracy:.4f} ({rf_combined_accuracy*100:.2f}%)\")\n",
    "print(f\"F1-Score (macro): {rf_combined_f1_macro:.4f}\")\n",
    "print(f\"Improvement over text-only: +{(rf_combined_accuracy - rf_text_accuracy)*100:.2f}%\")\n",
    "print(f\"Improvement over baseline: +{(rf_combined_accuracy - baseline_accuracy)*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Model Training - XGBoost"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train XGBoost - Text + Metadata\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBOOST - TEXT + METADATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTraining XGBoost on combined features...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "gbm = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_child_weight=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    tree_method='hist',  # Faster histogram-based algorithm\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "# XGBoost handles sparse matrices natively - no conversion needed!\n",
    "# Note: XGBoost expects classes to be 0-indexed, so we need to subtract 1\n",
    "y_train_xgb = y_train - 1\n",
    "y_test_xgb = y_test - 1\n",
    "\n",
    "gbm.fit(X_train_combined, y_train_xgb)\n",
    "\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "\n",
    "# Predictions (convert back to 1-5 scale)\n",
    "y_pred_gbm = gbm.predict(X_test_combined) + 1\n",
    "\n",
    "# Metrics\n",
    "gbm_accuracy = accuracy_score(y_test, y_pred_gbm)\n",
    "gbm_f1_macro = f1_score(y_test, y_pred_gbm, average='macro')\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"Accuracy: {gbm_accuracy:.4f} ({gbm_accuracy*100:.2f}%)\")\n",
    "print(f\"F1-Score (macro): {gbm_f1_macro:.4f}\")\n",
    "print(f\"Improvement over RF combined: {(gbm_accuracy - rf_combined_accuracy)*100:+.2f}%\")\n",
    "print(f\"Improvement over baseline: +{(gbm_accuracy - baseline_accuracy)*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create comprehensive comparison\nprint(\"=\" * 80)\nprint(\"MODEL COMPARISON SUMMARY\")\nprint(\"=\" * 80)\n\n# Store all results\nresults = {\n    'Model': [\n        'Baseline (Most Frequent)',\n        'Random Forest (Text Only)',\n        'Random Forest (Text + Metadata)',\n        'XGBoost (Text + Metadata)'\n    ],\n    'Accuracy': [\n        baseline_accuracy,\n        rf_text_accuracy,\n        rf_combined_accuracy,\n        gbm_accuracy\n    ],\n    'F1 (Macro)': [\n        0.0,  # Baseline has no real F1\n        rf_text_f1_macro,\n        rf_combined_f1_macro,\n        gbm_f1_macro\n    ]\n}\n\nresults_df = pd.DataFrame(results)\nprint(\"\\n\", results_df.to_string(index=False))\n\n# Key insights\nmetadata_improvement = rf_combined_accuracy - rf_text_accuracy\nbest_model = results_df.loc[results_df['Accuracy'].idxmax(), 'Model']\nbest_accuracy = results_df['Accuracy'].max()\n\nprint(f\"\\n\" + \"=\" * 80)\nprint(\"KEY FINDINGS\")\nprint(\"=\" * 80)\nprint(f\"\\n1. Adding metadata improves accuracy by: {metadata_improvement*100:+.2f}%\")\nprint(f\"2. Best model: {best_model}\")\nprint(f\"3. Best accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\nprint(f\"4. Total improvement over baseline: {(best_accuracy - baseline_accuracy)*100:.2f}%\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].barh(results_df['Model'], results_df['Accuracy'], color=['gray', 'skyblue', 'steelblue', 'darkblue'])\n",
    "axes[0].set_xlabel('Accuracy', fontweight='bold')\n",
    "axes[0].set_title('Model Accuracy Comparison', fontweight='bold', fontsize=14)\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    axes[0].text(v + 0.01, i, f'{v:.3f} ({v*100:.1f}%)', va='center')\n",
    "\n",
    "# F1 Score comparison (excluding baseline)\n",
    "results_df_no_baseline = results_df[results_df['Model'] != 'Baseline (Most Frequent)']\n",
    "axes[1].barh(results_df_no_baseline['Model'], results_df_no_baseline['F1 (Macro)'], \n",
    "            color=['skyblue', 'steelblue', 'darkblue'])\n",
    "axes[1].set_xlabel('F1 Score (Macro Average)', fontweight='bold')\n",
    "axes[1].set_title('Model F1 Score Comparison', fontweight='bold', fontsize=14)\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(results_df_no_baseline['F1 (Macro)']):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Outputs/Plots/08_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved: 08_model_comparison.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Classification reports for each model\nprint(\"=\" * 80)\nprint(\"DETAILED CLASSIFICATION REPORTS\")\nprint(\"=\" * 80)\n\nmodels_for_report = [\n    ('Random Forest (Text Only)', y_pred_rf_text),\n    ('Random Forest (Text + Metadata)', y_pred_rf_combined),\n    ('XGBoost (Text + Metadata)', y_pred_gbm)\n]\n\nfor model_name, y_pred in models_for_report:\n    print(f\"\\n{'='*80}\")\n    print(f\"{model_name}\")\n    print('='*80)\n    print(classification_report(y_test, y_pred, \n                                target_names=['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars']))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Per-class metrics comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate per-class metrics for best model (XGBoost)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred_gbm)\n",
    "\n",
    "per_class_df = pd.DataFrame({\n",
    "    'Star Rating': [1, 2, 3, 4, 5],\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nXGBoost - Per-Class Metrics:\")\n",
    "print(per_class_df.to_string(index=False))\n",
    "\n",
    "# Identify hardest class\n",
    "hardest_class = per_class_df.loc[per_class_df['F1-Score'].idxmin(), 'Star Rating']\n",
    "hardest_f1 = per_class_df.loc[per_class_df['F1-Score'].idxmin(), 'F1-Score']\n",
    "easiest_class = per_class_df.loc[per_class_df['F1-Score'].idxmax(), 'Star Rating']\n",
    "easiest_f1 = per_class_df.loc[per_class_df['F1-Score'].idxmax(), 'F1-Score']\n",
    "\n",
    "print(f\"\\nKey Observations:\")\n",
    "print(f\"Hardest to predict: {int(hardest_class)}-star reviews (F1: {hardest_f1:.3f})\")\n",
    "print(f\"Easiest to predict: {int(easiest_class)}-star reviews (F1: {easiest_f1:.3f})\")\n",
    "print(f\"Performance gap: {(easiest_f1 - hardest_f1):.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate confusion matrices\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models_for_cm = [\n",
    "    ('RF (Text Only)', y_pred_rf_text),\n",
    "    ('RF (Text + Meta)', y_pred_rf_combined),\n",
    "    ('XGBoost (Text + Meta)', y_pred_gbm)\n",
    "]\n",
    "\n",
    "for idx, (model_name, y_pred) in enumerate(models_for_cm):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Normalize for better visualization\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "               xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5],\n",
    "               ax=axes[idx], cbar_kws={'label': 'Proportion'})\n",
    "    axes[idx].set_xlabel('Predicted Rating', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual Rating', fontweight='bold')\n",
    "    axes[idx].set_title(f'{model_name}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Outputs/Plots/09_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved: 09_confusion_matrices.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Analyze confusion patterns\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFUSION MATRIX ANALYSIS (Best Model: XGBoost)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_gbm)\n",
    "\n",
    "print(\"\\nRaw confusion matrix:\")\n",
    "print(\"Predicted\")\n",
    "print(\"Actual   1   2   3   4   5\")\n",
    "for i, row in enumerate(cm, 1):\n",
    "    print(f\"  {i}   {row[0]:5d} {row[1]:5d} {row[2]:5d} {row[3]:5d} {row[4]:5d}\")\n",
    "\n",
    "# Calculate key confusion patterns\n",
    "print(\"\\nKey confusion patterns:\")\n",
    "for actual in range(5):\n",
    "    # Off-by-one errors (predicting 3 when actual is 4, etc.)\n",
    "    if actual > 0:\n",
    "        off_by_one_lower = cm[actual, actual-1]\n",
    "    else:\n",
    "        off_by_one_lower = 0\n",
    "    \n",
    "    if actual < 4:\n",
    "        off_by_one_upper = cm[actual, actual+1]\n",
    "    else:\n",
    "        off_by_one_upper = 0\n",
    "    \n",
    "    correct = cm[actual, actual]\n",
    "    total = cm[actual, :].sum()\n",
    "    off_by_one_total = off_by_one_lower + off_by_one_upper\n",
    "    \n",
    "    print(f\"\\n{actual+1} star reviews:\")\n",
    "    print(f\"Correctly predicted: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "    print(f\"Off by 1 star: {off_by_one_total}/{total} ({off_by_one_total/total*100:.1f}%)\")\n",
    "    print(f\"Off by 2+ stars: {total-correct-off_by_one_total}/{total} ({(total-correct-off_by_one_total)/total*100:.1f}%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature importance for Random Forest\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS - RANDOM FOREST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_combined.feature_importances_\n",
    "\n",
    "# Create feature names (TF-IDF terms + metadata)\n",
    "all_feature_names = list(feature_names) + metadata_feature_names\n",
    "\n",
    "# Create dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 30 Most Important Features:\")\n",
    "print(importance_df.head(30).to_string(index=False))\n",
    "\n",
    "# Separate text vs metadata importance\n",
    "n_text_features = len(feature_names)\n",
    "text_importance = importances[:n_text_features].sum()\n",
    "metadata_importance = importances[n_text_features:].sum()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"IMPORTANCE BY FEATURE TYPE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal importance from text features: {text_importance:.4f} ({text_importance*100:.1f}%)\")\n",
    "print(f\"Total importance from metadata: {metadata_importance:.4f} ({metadata_importance*100:.1f}%)\")\n",
    "print(f\"\\nText features are {text_importance/metadata_importance:.1f}x more important than metadata\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize top features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Top 20 overall features\n",
    "top_20 = importance_df.head(20)\n",
    "axes[0].barh(range(20), top_20['Importance'].values[::-1], color='steelblue')\n",
    "axes[0].set_yticks(range(20))\n",
    "axes[0].set_yticklabels(top_20['Feature'].values[::-1])\n",
    "axes[0].set_xlabel('Importance', fontweight='bold')\n",
    "axes[0].set_title('Top 20 Most Important Features (RF)', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Text vs Metadata importance\n",
    "importance_by_type = pd.DataFrame({\n",
    "    'Type': ['Text Features', 'Metadata Features'],\n",
    "    'Total Importance': [text_importance, metadata_importance]\n",
    "})\n",
    "\n",
    "axes[1].bar(importance_by_type['Type'], importance_by_type['Total Importance'], \n",
    "           color=['skyblue', 'coral'], edgecolor='black')\n",
    "axes[1].set_ylabel('Total Importance', fontweight='bold')\n",
    "axes[1].set_title('Feature Importance by Type', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(importance_by_type['Total Importance']):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}\\n({v*100:.1f}%)', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Outputs/Plots/10_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved: 10_feature_importance.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Error analysis - find worst predictions\n",
    "print(\"=\" * 80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create error analysis dataframe\n",
    "test_df_copy = test_df.copy().reset_index(drop=True)\n",
    "test_df_copy['predicted'] = y_pred_gbm\n",
    "test_df_copy['actual'] = y_test\n",
    "test_df_copy['error'] = test_df_copy['predicted'] - test_df_copy['actual']\n",
    "test_df_copy['abs_error'] = abs(test_df_copy['error'])\n",
    "test_df_copy['correct'] = test_df_copy['predicted'] == test_df_copy['actual']\n",
    "\n",
    "# Overall error statistics\n",
    "print(\"\\nOverall Error Statistics:\")\n",
    "print(f\"Correct predictions: {test_df_copy['correct'].sum():,} ({test_df_copy['correct'].mean()*100:.2f}%)\")\n",
    "print(f\"Incorrect predictions: {(~test_df_copy['correct']).sum():,} ({(~test_df_copy['correct']).mean()*100:.2f}%)\")\n",
    "print(f\"\\nMean absolute error: {test_df_copy['abs_error'].mean():.3f} stars\")\n",
    "print(f\"Median absolute error: {test_df_copy['abs_error'].median():.1f} stars\")\n",
    "\n",
    "# Error by magnitude\n",
    "print(\"\\nError by Magnitude:\")\n",
    "for error in range(0, 5):\n",
    "    count = (test_df_copy['abs_error'] == error).sum()\n",
    "    pct = count / len(test_df_copy) * 100\n",
    "    print(f\"  Off by {error} stars: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Error by actual rating\n",
    "print(\"\\nAccuracy by Actual Star Rating:\")\n",
    "for star in [1, 2, 3, 4, 5]:\n",
    "    mask = test_df_copy['actual'] == star\n",
    "    accuracy = test_df_copy[mask]['correct'].mean()\n",
    "    count = mask.sum()\n",
    "    print(f\"  {star} star: {accuracy*100:.1f}% correct ({count:,} reviews)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show specific error examples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE MISCLASSIFICATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extreme errors (predicted 5, actual 1 and vice versa)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTREME ERRORS (Predicted 5★, Actual 1★)\")\n",
    "print(\"=\"*80)\n",
    "extreme_errors_5_1 = test_df_copy[(test_df_copy['predicted'] == 5) & (test_df_copy['actual'] == 1)]\n",
    "if len(extreme_errors_5_1) > 0:\n",
    "    for idx, row in extreme_errors_5_1.head(3).iterrows():\n",
    "        print(f\"\\nExample {idx+1}:\")\n",
    "        print(f\"Review: {row['text'][:300]}...\")\n",
    "        print(f\"Word count: {row['word_count']}\")\n",
    "        if 'categories' in row:\n",
    "            print(f\"Category: {row['categories'][:50]}...\")\n",
    "else:\n",
    "    print(\"No examples found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTREME ERRORS (Predicted 1 star, Actual 5 star)\")\n",
    "print(\"=\"*80)\n",
    "extreme_errors_1_5 = test_df_copy[(test_df_copy['predicted'] == 1) & (test_df_copy['actual'] == 5)]\n",
    "if len(extreme_errors_1_5) > 0:\n",
    "    for idx, row in extreme_errors_1_5.head(3).iterrows():\n",
    "        print(f\"\\nExample {idx+1}:\")\n",
    "        print(f\"Review: {row['text'][:300]}...\")\n",
    "        print(f\"Word count: {row['word_count']}\")\n",
    "        if 'categories' in row:\n",
    "            print(f\"  Category: {row['categories'][:50]}...\")\n",
    "else:\n",
    "    print(\"No examples found\")\n",
    "\n",
    "# 3-star confusion (predicted 2 or 4, actual 3)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3-STAR CONFUSION (Actual 3 star, but predicted 2 star or 4 star)\")\n",
    "print(\"=\"*80)\n",
    "three_star_errors = test_df_copy[(test_df_copy['actual'] == 3) & \n",
    "                                 (test_df_copy['predicted'].isin([2, 4]))]\n",
    "if len(three_star_errors) > 0:\n",
    "    for idx, row in three_star_errors.head(3).iterrows():\n",
    "        print(f\"\\nExample {idx+1}: Predicted {int(row['predicted'])}★, Actual {int(row['actual'])}★\")\n",
    "        print(f\"Review: {row['text'][:300]}...\")\n",
    "        print(f\"Word count: {row['word_count']}\")\n",
    "else:\n",
    "    print(\"No examples found\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Error magnitude distribution\n",
    "error_counts = test_df_copy['abs_error'].value_counts().sort_index()\n",
    "axes[0].bar(error_counts.index, error_counts.values, color='coral', edgecolor='black')\n",
    "axes[0].set_xlabel('Absolute Error (stars)', fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontweight='bold')\n",
    "axes[0].set_title('Distribution of Prediction Errors', fontweight='bold')\n",
    "axes[0].set_xticks([0, 1, 2, 3, 4])\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (error, count) in enumerate(error_counts.items()):\n",
    "    pct = count / len(test_df_copy) * 100\n",
    "    axes[0].text(error, count, f'{count:,}\\n({pct:.1f}%)', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Accuracy by star rating\n",
    "accuracy_by_star = test_df_copy.groupby('actual')['correct'].mean()\n",
    "axes[1].bar(accuracy_by_star.index, accuracy_by_star.values, color='steelblue', edgecolor='black')\n",
    "axes[1].set_xlabel('Actual Star Rating', fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[1].set_title('Model Accuracy by Star Rating', fontweight='bold')\n",
    "axes[1].set_xticks([1, 2, 3, 4, 5])\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].axhline(y=gbm_accuracy, color='red', linestyle='--', \n",
    "               label=f'Overall Accuracy: {gbm_accuracy:.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Add percentage labels\n",
    "for star, acc in accuracy_by_star.items():\n",
    "    axes[1].text(star, acc + 0.02, f'{acc:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Outputs/Plots/11_error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved: 11_error_analysis.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save trained models\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING MODELS AND RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nSaving trained models...\")\n",
    "\n",
    "# Save models\n",
    "with open('../Outputs/Models/rf_text.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_text, f)\n",
    "print(\"Saved: rf_text.pkl\")\n",
    "\n",
    "with open('../Outputs/Models/rf_combined.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_combined, f)\n",
    "print(\"Saved: rf_combined.pkl\")\n",
    "\n",
    "with open('../Outputs/Models/gbm.pkl', 'wb') as f:\n",
    "    pickle.dump(gbm, f)\n",
    "print(\"Saved: gbm.pkl\")\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "with open('../Outputs/Models/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "print(\"Saved: tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Save metadata encoders\n",
    "with open('../Outputs/Models/metadata_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata_encoders, f)\n",
    "print(\"Saved: metadata_encoders.pkl\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save results summary\n",
    "print(\"\\nSaving results summary...\")\n",
    "\n",
    "# Create comprehensive results dictionary\n",
    "results_summary = {\n",
    "    'dataset_info': {\n",
    "        'train_size': len(train_df),\n",
    "        'test_size': len(test_df),\n",
    "        'n_features_text': X_train_text.shape[1],\n",
    "        'n_features_metadata': X_train_meta.shape[1],\n",
    "        'n_features_total': X_train_combined.shape[1]\n",
    "    },\n",
    "    'baseline': {\n",
    "        'accuracy': float(baseline_accuracy),\n",
    "        'method': 'Most frequent class'\n",
    "    },\n",
    "    'random_forest_text': {\n",
    "        'accuracy': float(rf_text_accuracy),\n",
    "        'f1_macro': float(rf_text_f1_macro)\n",
    "    },\n",
    "    'random_forest_combined': {\n",
    "        'accuracy': float(rf_combined_accuracy),\n",
    "        'f1_macro': float(rf_combined_f1_macro)\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'accuracy': float(gbm_accuracy),\n",
    "        'f1_macro': float(gbm_f1_macro)\n",
    "    },\n",
    "    'metadata_improvement': float(metadata_improvement),\n",
    "    'feature_importance': {\n",
    "        'text_total': float(text_importance),\n",
    "        'metadata_total': float(metadata_importance),\n",
    "        'top_20_features': importance_df.head(20).to_dict('records')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "with open('../Outputs/results_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(\"Saved: results_summary.json\")\n",
    "\n",
    "# Save results dataframe as CSV\n",
    "results_df.to_csv('../Outputs/model_comparison.csv', index=False)\n",
    "print(\"Saved: model_comparison.csv\")\n",
    "\n",
    "# Save per-class performance\n",
    "per_class_df.to_csv('../Outputs/per_class_performance.csv', index=False)\n",
    "print(\"Saved: per_class_performance.csv\")\n",
    "\n",
    "# Save error analysis\n",
    "error_summary = test_df_copy[['text', 'actual', 'predicted', 'error', 'abs_error', 'word_count']]\n",
    "error_summary.to_csv('../Outputs/error_analysis.csv', index=False)\n",
    "print(\"Saved: error_analysis.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL MODELS AND RESULTS SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
